{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 08. RNNs & LSTMs\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/pytorch_tutorial/blob/main/08_rnn_lstm/demo.ipynb)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Character-Level Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample text\n",
        "text = \"hello world, this is a simple example of character level language model.\"\n",
        "chars = sorted(set(text))\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
        "vocab_size = len(chars)\n",
        "print(f'Vocab size: {vocab_size}')\n",
        "print(f'Characters: {chars}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=32, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, x, hidden=None):\n",
        "        embed = self.embedding(x)\n",
        "        output, hidden = self.lstm(embed, hidden)\n",
        "        logits = self.fc(output)\n",
        "        return logits, hidden\n",
        "\n",
        "model = CharLSTM(vocab_size)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data\n",
        "seq_len = 20\n",
        "data = torch.tensor([char_to_idx[c] for c in text])\n",
        "\n",
        "# Create sequences\n",
        "X, Y = [], []\n",
        "for i in range(len(data) - seq_len):\n",
        "    X.append(data[i:i+seq_len])\n",
        "    Y.append(data[i+1:i+seq_len+1])\n",
        "X = torch.stack(X)\n",
        "Y = torch.stack(Y)\n",
        "print(f'X shape: {X.shape}, Y shape: {Y.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    logits, _ = model(X)\n",
        "    loss = criterion(logits.view(-1, vocab_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text\n",
        "model.eval()\n",
        "start = 'hello'\n",
        "generated = list(start)\n",
        "hidden = None\n",
        "\n",
        "x = torch.tensor([[char_to_idx[c] for c in start]])\n",
        "with torch.no_grad():\n",
        "    for _ in range(50):\n",
        "        logits, hidden = model(x, hidden)\n",
        "        probs = F.softmax(logits[0, -1], dim=-1)\n",
        "        next_idx = torch.multinomial(probs, 1).item()\n",
        "        generated.append(idx_to_char[next_idx])\n",
        "        x = torch.tensor([[next_idx]])\n",
        "\n",
        "print('Generated:', ''.join(generated))"
      ]
    }
  ]
}