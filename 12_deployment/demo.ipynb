{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 12. Deployment\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gaurav-redhat/pytorch_tutorial/blob/main/12_deployment/demo.ipynb)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TorchScript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 5)\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Trace\n",
        "example = torch.randn(1, 10)\n",
        "traced = torch.jit.trace(model, example)\n",
        "\n",
        "# Save\n",
        "traced.save('model_traced.pt')\n",
        "print('Saved traced model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and use\n",
        "loaded = torch.jit.load('model_traced.pt')\n",
        "output = loaded(torch.randn(5, 10))\n",
        "print(f'Output shape: {output.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dynamic quantization\n",
        "quantized = torch.quantization.quantize_dynamic(\n",
        "    model, {nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Compare sizes\n",
        "import os\n",
        "torch.save(model.state_dict(), 'model_fp32.pt')\n",
        "torch.save(quantized.state_dict(), 'model_int8.pt')\n",
        "print(f'FP32 size: {os.path.getsize(\"model_fp32.pt\")/1024:.1f} KB')\n",
        "print(f'INT8 size: {os.path.getsize(\"model_int8.pt\")/1024:.1f} KB')"
      ]
    }
  ]
}